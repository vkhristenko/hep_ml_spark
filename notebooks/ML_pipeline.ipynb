{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "* One-Hot-Encode the label (dim=3)\n",
    "* Split the dataset into training and validation (80/20%)\n",
    "* Scale the features to take values between 0 and 1 \n",
    "    * fit on training dataset and apply the scaler also to the validation set\n",
    "* Buld the NN:\n",
    "    * 3 hidden layers with 50, 20 and 10 nodes activated by *ReLU*\n",
    "    * Output layer with 3 nodes and *Softmax* activation\n",
    "    * Use *categorical crossentropy* as a loss\n",
    "    * Ask Maurizio for the optimizer, weight initialization, regularization, dropout\n",
    "    * For now we can use *Adam* and leave everything else as default\n",
    "* Create the trainer\n",
    "    * AEASGD for now is the one with the best performances\n",
    "* Train the model!\n",
    "\n",
    "**Optional**\n",
    "* ... Example of Cross Validation using spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/afs/cern.ch/work/m/migliori/public/spark2.3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_name = 'dist-keras-notebook'\n",
    "master = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.diana-hep:spark-root_2.11:0.1.14 pyspark-shell\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"test-spark-root\")\\\n",
    "        .config(\"spark.driver.memory\", \"15G\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316712"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HLF_dataset = spark.read.format(\"parquet\").load(\"HLF_dataset.parquet\")\n",
    "HLF_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           hfeatures|label|\n",
      "+--------------------+-----+\n",
      "|[405.151287078857...|    1|\n",
      "|[53.2725028991699...|    1|\n",
      "|[323.423492431640...|    1|\n",
      "|[228.913959503173...|    1|\n",
      "|[149.133995056152...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HLF_dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converte hfeatures in vector dense \n",
    "## The function used to convert returns a list but we need vector dense\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "vector_dense_udf = udf(lambda r : Vectors.dense(r),VectorUDT())\n",
    "HLF_dataset = HLF_dataset.withColumn('dense_features',vector_dense_udf('hfeatures'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train and test dataframes\n",
    "train, test = HLF_dataset.randomSplit([0.8, 0.2], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "## One-Hot-Encode\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"label\"], outputCols=[\"encoded_label\"])\n",
    "\n",
    "## Scale feature vector\n",
    "scaler = MinMaxScaler(inputCol=\"dense_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, scaler])\n",
    "\n",
    "fitted_pipeline = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform train and test\n",
    "train = fitted_pipeline.transform(train)\n",
    "test = fitted_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|        label|\n",
      "+--------------------+-------------+\n",
      "|[0.0,0.0155206299...|(2,[1],[1.0])|\n",
      "|[0.0,0.0190699355...|(2,[1],[1.0])|\n",
      "|[0.0,0.0190699355...|(2,[1],[1.0])|\n",
      "|[0.0,0.0200420945...|(2,[1],[1.0])|\n",
      "|[0.0,0.0200420945...|(2,[1],[1.0])|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.selectExpr('features', 'encoded_label as label')\n",
    "test = test.selectExpr('features', 'encoded_label as label')\n",
    "\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Keras model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
