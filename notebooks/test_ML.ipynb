{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/afs/cern.ch/work/m/migliori/public/spark2.3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of workers: 5\n"
     ]
    }
   ],
   "source": [
    "application_name = 'dist-keras-notebook'\n",
    "master = \"local[*]\"\n",
    "num_executors = 5\n",
    "num_cores = 4\n",
    "\n",
    "## Map worker -> executor\n",
    "num_workers = num_executors\n",
    "print(\"Total number of workers: %d\" % (num_workers) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.diana-hep:spark-root_2.11:0.1.14 pyspark-shell\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"test-spark-root\")\\\n",
    "        .config(\"spark.driver.memory\", \"40G\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code on a cluster just change the SparkSession builder and the master\n",
    "\n",
    "```Python\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(application_name)\\\n",
    "        .config(\"spark.pyspark.python\", \"/afs/cern.ch/work/m/migliori/public/anaconda2/bin/python\")\\\n",
    "        .config(\"spark.master\", master)\\\n",
    "        .config(\"spark.executor.cores\", '{}'.format(num_cores))\\\n",
    "        .config(\"spark.executor.instances\", '{}'.format(num_executors))\\\n",
    "        .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ithdp1063.cern.ch:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test-spark-root</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0641af7b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and convert the samples\n",
    "\n",
    "Create the vectors containing Low Level and High Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qcd ...\n",
      "There are 47952 qcd events\n",
      "Converting the dataframe ...\n",
      "Loading ttbar ...\n",
      "There are 479952 ttbar events\n",
      "Converting the dataframe ...\n",
      "Loading wjets ...\n",
      "There are 480000 wjets events\n",
      "Converting the dataframe ...\n",
      "\n",
      "Loading and Converting the samples took 19 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "import time\n",
    "\n",
    "from Utils_functions import *\n",
    "\n",
    "PATH = 'data/small_sample/'\n",
    "samples = ['qcd', 'ttbar', 'wjets']\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "requiredColumns = [\"EFlowTrack\", \"MuonTight_size\", \"Electron_size\",\n",
    "                   \"EFlowNeutralHadron\", \"EFlowPhoton\", \"Electron\",\n",
    "                   \"MuonTight\", \"MissingET\", \"Jet\"]\n",
    "\n",
    "data = None\n",
    "\n",
    "start = time.time()\n",
    "for sample, label in zip(samples, labels):\n",
    "    print('Loading {} ...'.format(sample))\n",
    "    \n",
    "    #Load data in a temporary dataframe \n",
    "    df_tmp = spark.read \\\n",
    "        .format(\"org.dianahep.sparkroot.experimental\") \\\n",
    "        .load(PATH+sample+'*.root') \\\n",
    "        .select(requiredColumns).toDF(*requiredColumns)\n",
    "    \n",
    "    #Count how many events there are\n",
    "    print(\"There are {} {} events\".format(df_tmp.count(), sample))\n",
    "    \n",
    "    ## Convert the dataframe and add the label\n",
    "    print('Converting the dataframe ...')\n",
    "    df_tmp_featured = df_tmp.rdd\\\n",
    "                    .map(convert)\\\n",
    "                    .filter(lambda row: len(row) > 0)\\\n",
    "                    .toDF() \\\n",
    "                    .withColumn(\"label\", lit(label))\n",
    "    \n",
    "    # Merge the dataframe with the previous ones \n",
    "    if data==None:\n",
    "        data = df_tmp_featured\n",
    "    else:\n",
    "        data = data.union(df_tmp_featured)\n",
    "     \n",
    "    # delete the tmp dataframe\n",
    "    del df_tmp \n",
    "\n",
    "stop = time.time()\n",
    "print('\\nLoading and Converting the samples took {} s'.format(int(stop-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hfeatures: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- lfeatures: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           hfeatures|label|\n",
      "+--------------------+-----+\n",
      "|[0.0, 1.844219446...|    0|\n",
      "|[0.0, 28.26162528...|    0|\n",
      "|[41.8011283874511...|    0|\n",
      "|[84.4664764404296...|    0|\n",
      "|[0.0, 16.53163719...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the HLF dataset\n",
    "HLF_dataset = data.select(['hfeatures', 'label'])\n",
    "HLF_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} events'.format(HLF_dataset.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "**TO DO:**\n",
    "* Split the dataset into training and validation (80/20%)\n",
    "* One-Hot-Encode the label (dim=3)\n",
    "* Scale the features to take values between 0 and 1 \n",
    "    * Then apply the scaler also to the validation set\n",
    "* Buld the NN:\n",
    "    * 3 hidden layers with 50, 20 and 10 nodes activated by *ReLU*\n",
    "    * Output layer with 3 nodes and *Softmax* activation\n",
    "    * Use *categorical crossentropy* as a loss\n",
    "    * Ask Maurizio for the optimizer, weight initialization, regularization, dropout\n",
    "    * For now we can use *Adam* and leave everything else as default\n",
    "* Create the trainer\n",
    "    * AEASGD for now is the one with the best performances\n",
    "* Train the model!\n",
    "\n",
    "**Optional**\n",
    "* ... Example of Cross Validation using spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
